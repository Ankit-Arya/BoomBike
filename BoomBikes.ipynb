{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21be1c11",
   "metadata": {},
   "source": [
    "# PROBLEM STATEMENT\n",
    "A US bike-sharing provider BoomBikes has recently suffered considerable dips \n",
    "in their revenues due to the ongoing Corona pandemic. The company is finding \n",
    "it very difficult to sustain in the current market scenario. So, it has decided \n",
    "to come up with a mindful business plan to be able to accelerate its revenue as \n",
    "soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n",
    "\n",
    "# Business Goal\n",
    "\n",
    "We are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1931f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress Warnings and importing essential Libs\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading_Data\n",
    "df = pd.read_csv(\"day.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18067315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the head of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803d205",
   "metadata": {},
   "source": [
    "Description of following fields:\n",
    "\t\n",
    "\t- instant: record index\n",
    "\t- dteday : date\n",
    "\t- season : season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "\t- yr : year (0: 2018, 1:2019)\n",
    "\t- mnth : month ( 1 to 12)\n",
    "\t- holiday : weather day is a holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "\t- weekday : day of the week\n",
    "\t- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "\t+ weathersit : \n",
    "\t\t- 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "\t\t- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "\t\t- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "\t\t- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "\t- temp : temperature in Celsius\n",
    "\t- atemp: feeling temperature in Celsius\n",
    "\t- hum: humidity\n",
    "\t- windspeed: wind speed\n",
    "\t- casual: count of casual users\n",
    "\t- registered: count of registered users\n",
    "\t- cnt: count of total rental bikes including both casual and registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f281190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc91103",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecfece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89ea242",
   "metadata": {},
   "source": [
    "## Step 2: Visualising the Data\n",
    "\n",
    "If there is some obvious multicollinearity going on, this is the first place to catch it\n",
    "Here's where you'll also identify if some predictors directly have a strong association with the outcome variable\n",
    "We'll visualise our data using matplotlib and seaborn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a67870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c019ba4",
   "metadata": {},
   "source": [
    "# OBSERVATIONS:\n",
    "\n",
    "There are no null values present\n",
    "\n",
    "We have both numerical and categorical values\n",
    "\n",
    "instant column is index which does not have any significance in our analysis. So we'll drop the column\n",
    "\n",
    "The varibles casual and registered are summed up to get cnt which is our target variable. \n",
    "Also during prediction we will not be having these data, so we will drop these two variables \n",
    "which we are not going to use in the model.\n",
    "\n",
    "We don't find any peculiar outliers after analysing description of dataframe\n",
    "\n",
    "'cnt' /demand of shared bikes definitely has some sort of relationship with 'temp'/ 'apparent_temp'.\n",
    "\n",
    "We are going to use weekday varible which is derived from dteday, so we will not be using dteday and will drop it.\n",
    "\n",
    "temp and atemp are directly correlated among each other. We will use temp and drop atemp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565af3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop observed unnecesary columns instance, dteday, casual, registered and atemp\n",
    "df.drop(['instant', 'dteday','casual','registered','atemp'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd6ef89",
   "metadata": {},
   "source": [
    "# CATEGORICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b4ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the correaltion between continous varibales using heat map\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a48e4f",
   "metadata": {},
   "source": [
    "OBSERVATIONS:\n",
    "\n",
    "    A positive correalation observed between cnt and temp (0.63)\n",
    "    A Negative correlation observed for cnt with hum,holiday and windspeed (-0.099,-0.069 and -0.24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ca8827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoxPlot for categorical variables\n",
    "col = 3\n",
    "categorical_vars = ['season','yr','mnth','holiday','weekday','workingday','weathersit']\n",
    "row = len(categorical_vars)//col+1\n",
    "\n",
    "plt.figure(figsize=(15,12))\n",
    "for i in list(enumerate(categorical_vars)):\n",
    "    plt.subplot(row,col,i[0]+1)\n",
    "    sns.boxplot(x = i[1], y = 'cnt', data = df)\n",
    "    plt.xticks(rotation = 90)\n",
    "plt.tight_layout(pad = 1)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4089681",
   "metadata": {},
   "source": [
    "# DATA CREATION/DUMMY VARIABLES INFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1af7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following categorical variables are mapped according to Data Dictionary :season, year, month, weekday, weathersit\n",
    "\n",
    "df['season']=df['season'].map({1: 'spring', 2: 'summer',3:'fall', 4:'winter'})\n",
    "df['yr']=df['yr'].map({0: '2018', 1: '2019'})\n",
    "df['mnth']=df['mnth'].map({1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'June', 7:'July', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'})\n",
    "df['weekday']=df['weekday'].map({0:'Sun', 1:'Mon', 2:'Tue', 3:'Wed', 4:'Thu', 5:'Fri', 6:'Sat'})\n",
    "df['weathersit']=df['weathersit'].map({1: 'Clear/Partly Cloudy', 2:'Mist/Cloudy', 3:'Light Snow/Rain', 4:'Heavy Snow/Rain/Hail/Fog'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9e7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummy variables for season, weekday, mnth and weathersit\n",
    "dummy_vars = pd.get_dummies(df[['season','weekday','mnth','weathersit','yr']],drop_first=True)\n",
    "\n",
    "# concat the dummy df with original df\n",
    "df = pd.concat([df,dummy_vars], axis = 1)\n",
    "\n",
    "# drop season column\n",
    "df.drop(['season','weekday','mnth','weathersit','yr'], axis=1, inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c902bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical columns to numeric \n",
    "df[['holiday','workingday']]= df[['holiday','workingday']].astype('uint8')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46df6d1",
   "metadata": {},
   "source": [
    "# DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1860b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train-test dataset\n",
    "df_train, df_test = train_test_split(df, train_size = 0.7, random_state = 100 )\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80276f8",
   "metadata": {},
   "source": [
    "Min-Max Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e38323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling of train set\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform on training data\n",
    "df_train[['temp', 'hum','windspeed','cnt']] = scaler.fit_transform(df_train[['temp', 'hum','windspeed','cnt']])\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only transform test dataset \n",
    "df_test[['temp', 'hum','windspeed','cnt']] = scaler.transform(df_test[['temp', 'hum','windspeed','cnt']])\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441496ab",
   "metadata": {},
   "source": [
    "SPLIT DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cce6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y data dataframe for train set\n",
    "y_train = df_train.pop('cnt')\n",
    "X_train = df_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9952ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y data dataframe for test set\n",
    "y_test = df_test.pop('cnt')\n",
    "X_test = df_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67778f4",
   "metadata": {},
   "source": [
    "DATA MODELLING USING RFE AND MANUAL SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RFE to select 15 number of varibleslm = LinearRegression()\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "rfe = RFE(lm,  n_features_to_select=15)\n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb7e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = X_train.columns[rfe.support_]\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa751ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUILDING USING RFE COLUMNS\n",
    "\n",
    "X_train_rfe = X_train[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43c5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for stats linear model \n",
    "def sm_linearmodel(X_train_sm):\n",
    "    #Add constant\n",
    "    X_train_sm = sm.add_constant(X_train_sm)\n",
    "\n",
    "    # create a fitted model (1st model)\n",
    "    lm = sm.OLS(y_train,X_train_sm).fit()\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99271e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate VIF\n",
    "# calculate VIF\n",
    "def vif_calc(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif['Features'] = X.columns\n",
    "    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif['VIF'] = round(vif['VIF'],2)\n",
    "    vif = vif.sort_values(by='VIF', ascending = False)\n",
    "    return vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3c9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1st stats model and look for summary and VIF\n",
    "lm_1 = sm_linearmodel(X_train_rfe)\n",
    "print(lm_1.summary())\n",
    "\n",
    "# Calculate VIF\n",
    "print(vif_calc(X_train_rfe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25272046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to remove P value variables >0.05 in bstep mannen and update model\n",
    "\n",
    "pvalue = lm_1.pvalues\n",
    "while(max(pvalue)>0.05):\n",
    "    maxp_var = pvalue[pvalue == pvalue.max()].index\n",
    "    print('Removed variable:' , maxp_var[0], '    P value: ', round(max(pvalue),3))\n",
    "    \n",
    "    # drop variable with high p value\n",
    "    X_train_rfe = X_train_rfe.drop(maxp_var, axis = 1)\n",
    "    lm_1 = sm_linearmodel(X_train_rfe)\n",
    "    pvalue = lm_1.pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb4f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm_1.summary())\n",
    "\n",
    "print(vif_calc(X_train_rfe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop varible having high VIF\n",
    "X_train_new = X_train_rfe.drop(['hum'],axis = 1)\n",
    "\n",
    "# Create stats model and look for summary\n",
    "lm_2 = sm_linearmodel(X_train_new)\n",
    "print(lm_2.summary())\n",
    "\n",
    "# Calculate VIF\n",
    "print(vif_calc(X_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e32bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop varible having high VIF\n",
    "X_train_new = X_train_new.drop(['workingday'],axis = 1)\n",
    "\n",
    "# Create stats model and look for summary\n",
    "lm_2 = sm_linearmodel(X_train_new)\n",
    "print(lm_2.summary())\n",
    "\n",
    "# Calculate VIF\n",
    "print(vif_calc(X_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8148f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop varible having high VIF\n",
    "X_train_new = X_train_new.drop(['temp'],axis = 1)\n",
    "\n",
    "# Create stats model and look for summary\n",
    "lm_3 = sm_linearmodel(X_train_new)\n",
    "print(lm_3.summary())\n",
    "\n",
    "# Calculate VIF\n",
    "print(vif_calc(X_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop varible having high VIF\n",
    "X_train_new = X_train_new.drop(['weekday_Sat'],axis = 1)\n",
    "\n",
    "# Create stats model and look for summary\n",
    "lm_4 = sm_linearmodel(X_train_new)\n",
    "print(lm_4.summary())\n",
    "\n",
    "# Calculate VIF\n",
    "print(vif_calc(X_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be8fbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop varible having high VIF\n",
    "X_train_new = X_train_new.drop(['mnth_July'],axis = 1)\n",
    "\n",
    "# Create stats model and look for summary\n",
    "lm_5 = sm_linearmodel(X_train_new)\n",
    "print(lm_5.summary())\n",
    "\n",
    "# Calculate VIF\n",
    "print(vif_calc(X_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daf2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop varible having high VIF\n",
    "# X_train_new = X_train_new.drop(['hum'],axis = 1)\n",
    "\n",
    "# # Create stats model and look for summary\n",
    "# lm_6 = sm_linearmodel(X_train_new)\n",
    "# print(lm_6.summary())\n",
    "\n",
    "# # Calculate VIF\n",
    "# print(vif_calc(X_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b759493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List down final model varibales and its coefficients\n",
    "\n",
    "# assign final model to lm_final\n",
    "lm_final = lm_5\n",
    "\n",
    "# list down and check variables of final model\n",
    "var_final = list(lm_final.params.index)\n",
    "var_final.remove('const')\n",
    "print('Final Selected Variables:', var_final)\n",
    "\n",
    "# Print the coefficents of final varible\n",
    "print('\\033[1m{:10s}\\033[0m'.format('\\nCoefficent for the variables are:'))\n",
    "print(round(lm_final.params,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1f182b",
   "metadata": {},
   "source": [
    "RESIDUAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final variables from the test dataset\n",
    "X_train_res = X_train[var_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ef1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add constant\n",
    "X_train_res = sm.add_constant(X_train_res)\n",
    "\n",
    "#Predict train set\n",
    "y_train_pred = lm_final.predict(X_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eda02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distrubition plot for residue\n",
    "res = y_train - y_train_pred\n",
    "sns.distplot(res)\n",
    "plt.title('Error terms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604379ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error terms train set\n",
    "c = [i for i in range(1,len(y_train)+1,1)]\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_train,res)\n",
    "fig.suptitle('Error Terms', fontsize=16)            # Plot heading \n",
    "plt.xlabel('Y_train_pred', fontsize=14)             # X-label\n",
    "plt.ylabel('Residual', fontsize=14) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126f1aa",
   "metadata": {},
   "source": [
    "PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf03cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select final variables from X_test\n",
    "X_test_sm = X_test[var_final]\n",
    "X_test_sm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104365c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add constant\n",
    "X_test_sm = sm.add_constant(X_test_sm)\n",
    "X_test_sm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict test dataset\n",
    "y_test_pred = lm_final.predict(X_test_sm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c063038",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676279bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get R-Squared fro test dataset\n",
    "r2_test = r2_score(y_true = y_test, y_pred = y_test_pred)\n",
    "print('R-Squared for Test dataset: ', round(r2_test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adj. R-Squared for test dataset\n",
    "N= len(X_test)          # sample size\n",
    "p =len(var_final)     # Number of independent variable\n",
    "r2_test_adj = round((1-((1-r2_test)*(N-1)/(N-p-1))),3)\n",
    "print('Adj. R-Squared for Test dataset: ', round(r2_test_adj,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bceedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_test = y_test - y_test_pred\n",
    "plt.title('Error Terms', fontsize=16) \n",
    "sns.distplot(res_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f5de99",
   "metadata": {},
   "source": [
    "## QnA's\n",
    "\n",
    "1. From your analysis of the categorical variables from the dataset, what could you infer about \n",
    "their effect on the dependent variable?\n",
    "\n",
    "CRITICAL OBSERVATIONS\n",
    "\n",
    "The demad of bike is less in the season 1 ie spring\n",
    "The demand bike increased in the year 2019 wrt year 2018.\n",
    "Month Jun - Sep is the period when bike demand is high. The Months Jan-Feb have lowest demand.\n",
    "Bike demand is less in holidays .\n",
    "\n",
    "\n",
    "2. Why is it important to use drop_first=True during dummy variable creation? \n",
    "\n",
    "drop_first=True is important to use, as it helps in reducing the extra column created during dummy variable creation. Hence it reduces the correlations created among dummy variables.\n",
    "\n",
    "3. Looking at the pair-plot among the numerical variables, which one has the highest correlation \n",
    "with the target variable?\n",
    "\n",
    "'temp' and 'atemp'\n",
    "\n",
    "\n",
    "4. How did you validate the assumptions of Linear Regression after building the model on the \n",
    "training set? \n",
    "\n",
    "Assumption of Regression Model : \n",
    "•\tLinearity: The relationship between dependent and independent variables should be linear.\n",
    "\n",
    "•\tHomoscedasticity: Constant variance of the errors should be maintained.\n",
    "\n",
    "•\tMultivariate normality: Multiple Regression assumes that the residuals are normally distributed.\n",
    "\n",
    "•\tLack of Multicollinearity: It is assumed that there is little or no multicollinearity in the data.\n",
    "\n",
    "\n",
    "\n",
    "5. Based on the final model, which are the top 3 features contributing significantly towards \n",
    "explaining the demand of the shared bikes?\n",
    "\n",
    "Top picks would be:\n",
    "\n",
    "    'temp'\n",
    "    \n",
    "    'yr'\n",
    "    \n",
    "    'weathersit'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05e5d3dc",
   "metadata": {},
   "source": [
    "1. Explain the linear regression algorithm in detail.\n",
    "\n",
    "Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on – the kind of relationship between dependent and independent variables they are considering, and the number of independent variables getting used.\n",
    "Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x). So, this regression technique finds out a linear relationship between x (input) and y(output). Hence, the name is Linear Regression.\n",
    "In the figure above, X (input) is the work experience and Y (output) is the salary of a person. The regression line is the best fit line for our model.\n",
    "\n",
    "\n",
    "2. Explain the Anscombe’s quartet in detail. \n",
    "\n",
    "Anscombe’s quartet tells us about the importance of visualizing data before applying various algorithms to build models. This suggests the data features must be plotted to see the distribution of the samples that can help you identify the various anomalies present in the data (outliers, diversity of the data, linear separability of the data, etc.). Moreover, the linear regression can only be considered a fit for the data with linear relationships and is incapable of handling any other kind of data set. \n",
    "\n",
    "3. What is Pearson’s R? \n",
    "\n",
    "\n",
    "In statistics, the Pearson correlation coefficient (PCC), also referred to as Pearson's r, the Pearson product-moment correlation coefficient (PPMCC), or the bivariate correlation, is a measure of linear correlation between two sets of data. It is the covariance of two variables, divided by the product of their standard deviations; thus it is essentially a normalised measurement of the covariance, such that the result always has a value between −1 and 1.\n",
    "\n",
    "4. What is scaling? Why is scaling performed? What is the difference between normalized scaling \n",
    "and standardized scaling? \n",
    "\n",
    "It is a step of data Pre-Processing which is applied to independent variables to normalize the data within a particular range. It also helps in speeding up the calculations in an algorithm.\n",
    "\n",
    "Most of the times, collected data set contains features highly varying in magnitudes, units and range. If scaling is not done then algorithm only takes magnitude in account and not units hence incorrect modelling. To solve this issue, we have to do scaling to bring all the variables to the same level of magnitude.\n",
    "\n",
    "It is important to note that scaling just affects the coefficients and none of the other parameters like t-statistic, F-statistic, p-values, R-squared, etc.\n",
    "\n",
    "Normalization/Min-Max Scaling:\n",
    "It brings all of the data in the range of 0 and 1. sklearn.preprocessing.MinMaxScaler helps to implement normalization in python.\n",
    "\n",
    "Standardization Scaling:\n",
    "Standardization replaces the values by their Z scores. It brings all of the data into a standard normal distribution which has mean (μ) zero and standard deviation one (σ).\n",
    "\n",
    "sklearn.preprocessing.scale helps to implement standardization in python.\n",
    "One disadvantage of normalization over standardization is that it loses some information in the data, especially about outliers.\n",
    "\n",
    "\n",
    "\n",
    "5. You might have observed that sometimes the value of VIF is infinite. Why does this happen?\n",
    "\n",
    "An infinite value of VIF for a given independent variable indicates that it can be perfectly predicted by other variables in the model.\n",
    "\n",
    "\n",
    "\n",
    "6. What is a Q-Q plot? Explain the use and importance of a Q-Q plot in linear regression.\n",
    "\n",
    "Quantile-Quantile (Q-Q) plot, is a graphical tool to help us assess if a set of data plausibly came from some theoretical distribution such as a Normal, exponential or Uniform distribution. Also, it helps to determine if two data sets come from populations with a common distribution.\n",
    "\n",
    "This helps in a scenario of linear regression when we have training and test data set received separately and then we can confirm using Q-Q plot that both the data sets are from populations with same distributions.\n",
    "\n",
    "Few advantages:\n",
    "a) It can be used with sample sizes also\n",
    "\n",
    "b) Many distributional aspects like shifts in location, shifts in scale, changes in symmetry, and the presence of outliers can all be detected from this plot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
